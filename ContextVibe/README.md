# ContextVibe: å¤šæ¨¡æ€ä¼šè®®æƒ…æ„Ÿåˆ†æç³»ç»Ÿ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

ContextVibeæ˜¯ä¸€ä¸ªåŸºäºAMI Meeting Corpusçš„å¤šæ¨¡æ€ä¼šè®®æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨è®¡ç®—ä¼šè®®å‚ä¸è€…çš„**Valenceï¼ˆæ•ˆä»·ï¼‰**ã€**Arousalï¼ˆå”¤é†’åº¦ï¼‰**ã€**Energyï¼ˆäº¤äº’åŒæ­¥æ€§ï¼‰**å’Œ**Cohesionï¼ˆä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰**å››ä¸ªç»´åº¦çš„æƒ…æ„ŸæŒ‡æ ‡ã€‚

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ¨¡æ€æ•°æ®å¤„ç†**: æ”¯æŒéŸ³é¢‘ã€è§†é¢‘ã€æ–‡æœ¬æ•°æ®çš„è”åˆåˆ†æ
- **VAECæƒ…æ„Ÿè®¡ç®—**: å®ç°å››ä¸ªç»´åº¦çš„æƒ…æ„ŸæŒ‡æ ‡è®¡ç®—
- **å¤šäººå¯¹è¯åˆ†æ**: æ”¯æŒå¤šå‚ä¸è€…ä¼šè®®åœºæ™¯
- **å®æ—¶çŠ¶æ€ç›‘æ§**: æä¾›å‚ä¸è€…çŠ¶æ€ç¨³å®šæ€§å’Œå›¢é˜Ÿèå…¥åº¦åˆ†æ
- **è‡ªåŠ¨åŒ–æµç¨‹**: ä»æ•°æ®ä¸‹è½½åˆ°ç»“æœç”Ÿæˆçš„å…¨è‡ªåŠ¨å¤„ç†

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### åŒ…ç»“æ„

```
ContextVibe/
â”œâ”€â”€ contextvibe/                 # ä¸»åŒ…ç›®å½•
â”‚   â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–æ–‡ä»¶
â”‚   â”œâ”€â”€ cli.py                  # å‘½ä»¤è¡Œæ¥å£
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒè®¡ç®—æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ vae_calculator.py   # VAECè®¡ç®—æ ¸å¿ƒ
â”‚   â”œâ”€â”€ analysis/               # åˆ†ææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ multimodal_cohesion_analyzer.py  # å¤šæ¨¡æ€ä¸€è‡´æ€§åˆ†æå™¨
â”‚   â”œâ”€â”€ data/                   # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ organize_ami_data.py # æ•°æ®å½’ç±»è„šæœ¬
â”‚   â””â”€â”€ utils/                  # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ processor.py        # ä¸»å¤„ç†ç¨‹åº
â”œâ”€â”€ amicorpus/                  # AMIæ•°æ®é›†ç›®å½•
â”‚   â”œâ”€â”€ audio/                  # éŸ³é¢‘æ•°æ®
â”‚   â”‚   â”œâ”€â”€ close_talking/      # è¿‘è·ç¦»éº¦å…‹é£éŸ³é¢‘
â”‚   â”‚   â””â”€â”€ far_field/          # è¿œåœºéº¦å…‹é£éŸ³é¢‘
â”‚   â”œâ”€â”€ video/                  # è§†é¢‘æ•°æ®
â”‚   â”‚   â”œâ”€â”€ individual/         # ä¸ªäººè§†è§’è§†é¢‘
â”‚   â”‚   â””â”€â”€ room_view/          # æˆ¿é—´è§†è§’è§†é¢‘
â”‚   â”œâ”€â”€ annotations/            # æ ‡æ³¨æ•°æ®
â”‚   â”‚   â”œâ”€â”€ transcripts/        # è½¬å½•æ–‡æœ¬
â”‚   â”‚   â”œâ”€â”€ dialogue_acts/      # å¯¹è¯è¡Œä¸º
â”‚   â”‚   â”œâ”€â”€ emotions/           # æƒ…æ„ŸçŠ¶æ€
â”‚   â”‚   â””â”€â”€ gestures/           # æ‰‹åŠ¿æ ‡æ³¨
â”‚   â”œâ”€â”€ slides/                 # å¹»ç¯ç‰‡æ•°æ®
â”‚   â”œâ”€â”€ whiteboard/             # ç™½æ¿æ•°æ®
â”‚   â”œâ”€â”€ metadata/               # å…ƒæ•°æ®
â”‚   â”œâ”€â”€ sample_data/            # æ ·æœ¬æ•°æ®
â”‚   â”œâ”€â”€ multi_speaker/          # å¤šäººå¯¹è¯æ•°æ®
â”‚   â””â”€â”€ results/                # åˆ†æç»“æœ
â”œâ”€â”€ sample_main.py              # ç¤ºä¾‹ä¸»ç¨‹åº
â”œâ”€â”€ setup.py                    # åŒ…å®‰è£…é…ç½®
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ README.md                   # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ LICENSE                     # è®¸å¯è¯æ–‡ä»¶
â””â”€â”€ MANIFEST.in                 # åŒ…æ¸…å•æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½œä¸ºPythonåŒ…ä½¿ç”¨

#### 1. å®‰è£…ContextVibeåŒ…

```bash
# ä»æºç å®‰è£…
git clone <repository-url>
cd ContextVibe
pip install -e .

# æˆ–è€…ç›´æ¥å®‰è£…
pip install contextvibe
```

#### 2. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·

```bash
# è¿è¡Œå®Œæ•´æµç¨‹
contextvibe --full-pipeline

# ä¸‹è½½æ ·æœ¬æ•°æ®
contextvibe --download-sample

# å¤„ç†æ ·æœ¬æ•°æ®
contextvibe --process-sample

# åˆ†æå¤šäººå¯¹è¯
contextvibe --analyze-multi-speaker

# æŸ¥çœ‹å¸®åŠ©
contextvibe --help
```

#### 3. ä½¿ç”¨Python API

```python
from contextvibe import VAE_CCalculator, MultimodalCohesionAnalyzer

# åˆ›å»ºè®¡ç®—å™¨
calculator = VAE_CCalculator()

# è®¡ç®—æ–‡æœ¬æƒ…æ„Ÿ
scores = calculator.va_calculator.calculate_text_va("è¿™ä¸ªé¡¹ç›®å¾ˆæ£’ï¼")
print(f"Valence: {scores[0]:.3f}, Arousal: {scores[1]:.3f}")

# åˆ›å»ºåˆ†æå™¨
analyzer = MultimodalCohesionAnalyzer()
```

### æ–¹æ³•äºŒï¼šç›´æ¥è¿è¡Œç¤ºä¾‹

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd ContextVibe

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œç¤ºä¾‹ç¨‹åº
python sample_main.py
```

### æ–¹æ³•ä¸‰ï¼šä¼ ç»Ÿæ–¹å¼

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd ContextVibe

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# åˆ›å»ºAMIæ•°æ®ç›®å½•
mkdir -p amicorpus
```

### 2. æ•°æ®è·å–

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨æ ·æœ¬æ•°æ®ï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
```bash
python3 ami_processor.py --download_sample
```

#### æ–¹æ³•äºŒï¼šä¸‹è½½å®Œæ•´AMIæ•°æ®é›†
```bash
# è®¿é—®AMIå®˜æ–¹ç½‘ç«™è·å–æ•°æ®
# https://groups.inf.ed.ac.uk/ami/corpus/

# è¿è¡Œæ•°æ®å½’ç±»è„šæœ¬
python3 organize_ami_data.py
```

### 3. è¿è¡Œåˆ†æ

```bash
# è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
python3 ami_processor.py

# æŸ¥çœ‹ç»“æœ
ls amicorpus/results/
```

### 4. ä½¿ç”¨ç¤ºä¾‹ç¨‹åº

```bash
# è¿è¡Œç¤ºä¾‹ç¨‹åº
python sample_main.py
```

## ğŸ“Š VAECè®¡ç®—æ–¹æ³•è¯¦è§£

### 1. **Valenceï¼ˆæ•ˆä»·ï¼‰è®¡ç®—**

#### æ–‡æœ¬æ•ˆä»·
```python
# ä½¿ç”¨NLTK VADERè¿›è¡Œæƒ…æ„Ÿåˆ†æ
from nltk.sentiment import SentimentIntensityAnalyzer

def calculate_text_va(self, text: str) -> Tuple[float, float]:
    scores = self.sentiment_analyzer.polarity_scores(text)
    valence = scores['compound']  # VADER compoundåˆ†æ•°
    arousal = scores['pos'] - scores['neg']  # æ­£è´Ÿæƒ…æ„Ÿå·®å¼‚
    return valence, arousal
```

#### éŸ³é¢‘æ•ˆä»·
```python
# åŸºäºéŸ³é¢‘ç‰¹å¾è®¡ç®—æ•ˆä»·
def calculate_audio_va(self, audio_path: str) -> Tuple[float, float]:
    y, sr = librosa.load(audio_path, sr=None)
    
    # éŸ³è°ƒç‰¹å¾ (valenceç›¸å…³)
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    pitch_mean = np.mean(pitches[magnitudes > 0.1])
    
    # é¢‘è°±è´¨å¿ƒ (valenceç›¸å…³)
    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
    
    # å½’ä¸€åŒ–ç‰¹å¾
    valence = (pitch_mean / 1000.0 + spectral_centroid / 5000.0) / 2
    return np.clip(valence, -1, 1)
```

### 2. **Arousalï¼ˆå”¤é†’åº¦ï¼‰è®¡ç®—**

#### æ–‡æœ¬å”¤é†’åº¦
```python
# åŸºäºæƒ…æ„Ÿå¼ºåº¦è®¡ç®—å”¤é†’åº¦
arousal = scores['pos'] - scores['neg']  # æ­£è´Ÿæƒ…æ„Ÿå·®å¼‚
```

#### éŸ³é¢‘å”¤é†’åº¦
```python
# åŸºäºéŸ³é¢‘èƒ½é‡è®¡ç®—å”¤é†’åº¦
energy = np.mean(librosa.feature.rms(y=y))
arousal = energy / 0.1  # å½’ä¸€åŒ–
return np.clip(arousal, -1, 1)
```

### 3. **Energyï¼ˆäº¤äº’åŒæ­¥æ€§ï¼‰è®¡ç®—**

#### éŸ³é¢‘èƒ½é‡
```python
def calculate_audio_energy(self, audio_path: str) -> float:
    y, sr = librosa.load(audio_path, sr=None)
    
    # éŸ³é‡å¼ºåº¦
    rms = librosa.feature.rms(y=y)
    volume_intensity = np.mean(rms)
    
    # è¯­é€ŸèŠ‚å¥
    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
    speech_rhythm = tempo / 200.0
    
    # ç»¼åˆèƒ½é‡å¾—åˆ†
    energy = (volume_intensity + speech_rhythm) / 2
    return float(np.clip(energy, 0, 1))
```

#### è§†é¢‘èƒ½é‡
```python
def calculate_video_energy(self, video_path: str) -> float:
    # ä½¿ç”¨MediaPipeè¿›è¡Œé¢éƒ¨æ£€æµ‹
    mp_face_mesh = mp.solutions.face_mesh.FaceMesh()
    
    # åˆ†æé¢éƒ¨åŠ¨ä½œé¢‘ç‡
    motion_scores = []
    for frame in video_frames:
        results = mp_face_mesh.process(frame)
        if results.multi_face_landmarks:
            motion_score = self._calculate_facial_motion(landmarks)
            motion_scores.append(motion_score)
    
    return float(np.mean(motion_scores))
```

### 4. **Cohesionï¼ˆä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰è®¡ç®—**

#### ä¸ªäººå‰åè¯è¯­ä¸€è‡´æ€§
```python
def _calculate_personal_cohesion(self, session_data: Dict) -> float:
    # æŒ‰è¯´è¯è€…ç»„ç»‡æ–‡æœ¬
    speaker_texts = {}
    for item in session_data.get('items', []):
        if 'text' in item:
            speaker = item.get('speaker', 'unknown')
            if speaker not in speaker_texts:
                speaker_texts[speaker] = []
            speaker_texts[speaker].append(item['text'])
    
    # è®¡ç®—æƒ…æ„Ÿå˜åŒ–çš„æ ‡å‡†å·®ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
    for speaker, texts in speaker_texts.items():
        sentiments = []
        for text in texts:
            scores = self.sentiment_analyzer.polarity_scores(text)
            sentiments.append(scores['compound'])
        
        sentiment_std = np.std(sentiments)
        stability = 1.0 / (1.0 + sentiment_std)  # å½’ä¸€åŒ–
        personal_cohesions.append(stability)
    
    return np.mean(personal_cohesions)
```

#### ä¸ªäººä¸æ•´ä½“ç¯å¢ƒä¸€è‡´æ€§
```python
def _calculate_environmental_cohesion(self, session_data: Dict) -> float:
    # è®¡ç®—æ•´ä½“ç¯å¢ƒæƒ…æ„Ÿ
    all_sentiments = []
    for text in all_texts:
        scores = self.sentiment_analyzer.polarity_scores(text)
        all_sentiments.append(scores['compound'])
    
    env_sentiment = np.mean(all_sentiments)
    
    # è®¡ç®—æ¯ä¸ªè¯´è¯è€…ä¸ç¯å¢ƒçš„å·®å¼‚
    for speaker, texts in speaker_texts.items():
        speaker_sentiments = []
        for text in texts:
            scores = self.sentiment_analyzer.polarity_scores(text)
            speaker_sentiments.append(scores['compound'])
        
        speaker_avg = np.mean(speaker_sentiments)
        deviation = 1.0 - abs(speaker_avg - env_sentiment)  # å·®å¼‚è¶Šå°ï¼Œèå…¥åº¦è¶Šé«˜
        speaker_deviations.append(deviation)
    
    return np.mean(speaker_deviations)
```

## ğŸ­ å¤šäººå¤šæ¨¡æ€æ•°æ®åˆæˆ

### 1. **å¤šäººå¯¹è¯åœºæ™¯è®¾è®¡**

ç³»ç»Ÿæ”¯æŒ4ç§å…¸å‹çš„å¤šäººå¯¹è¯åœºæ™¯ï¼š

#### åœºæ™¯1ï¼šé«˜æ•ˆå›¢é˜Ÿè®¨è®º
```json
{
  "session_id": "meeting_productive",
  "description": "é«˜æ•ˆå›¢é˜Ÿè®¨è®ºï¼ŒçŠ¶æ€ç¨³å®šï¼Œèå…¥åº¦é«˜",
  "participants": ["Alice", "Bob", "Charlie"],
  "items": [
    {"speaker": "Alice", "text": "è¿™ä¸ªæ–¹æ¡ˆå¾ˆå¥½ï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³å®æ–½ã€‚"},
    {"speaker": "Bob", "text": "åŒæ„ï¼Œæ—¶é—´å®‰æ’ä¹Ÿå¾ˆåˆç†ã€‚"},
    {"speaker": "Charlie", "text": "æˆ‘æ¥è´Ÿè´£æŠ€æœ¯å®ç°éƒ¨åˆ†ã€‚"}
  ]
}
```

#### åœºæ™¯2ï¼šçŠ¶æ€ä¸‹æ»‘
```json
{
  "session_id": "meeting_stress",
  "description": "AliceçŠ¶æ€é€æ¸ä¸‹æ»‘ï¼Œå…¶ä»–äººä¿æŒç¨³å®š",
  "participants": ["Alice", "Bob", "Charlie"],
  "items": [
    {"speaker": "Alice", "text": "è¿™ä¸ªæƒ³æ³•å¾ˆæœ‰åˆ›æ„ã€‚"},
    {"speaker": "Alice", "text": "æˆ‘è§‰å¾—...å¯èƒ½æœ‰é—®é¢˜ã€‚"},
    {"speaker": "Alice", "text": "æˆ‘ä¸ç¡®å®šèƒ½å¦å®Œæˆ..."}
  ]
}
```

#### åœºæ™¯3ï¼šå›¢é˜Ÿå†²çª
```json
{
  "session_id": "meeting_conflict",
  "description": "Davidä¸å›¢é˜Ÿæ°›å›´ä¸åè°ƒ",
  "participants": ["Alice", "Bob", "Charlie", "David"],
  "items": [
    {"speaker": "David", "text": "è¿™ä¸ªæ–¹æ¡ˆå®Œå…¨ä¸å¯è¡Œã€‚"},
    {"speaker": "Alice", "text": "æˆ‘ä»¬å¯ä»¥è®¨è®ºæ”¹è¿›æ–¹æ¡ˆã€‚"},
    {"speaker": "David", "text": "æµªè´¹æ—¶é—´ï¼Œæˆ‘ä¸åŒæ„ã€‚"}
  ]
}
```

### 2. **å¤šæ¨¡æ€æ•°æ®èåˆ**

```python
def _calculate_multimodal_cohesion(self, results: Dict) -> Dict:
    text_cohesion = results.get('text_cohesion', {}).get('overall_cohesion', 0.0)
    audio_cohesion = results.get('audio_cohesion', {}).get('overall_audio_cohesion', 0.0)
    video_cohesion = results.get('video_cohesion', {}).get('overall_video_cohesion', 0.0)
    
    # æƒé‡é…ç½®
    multimodal_cohesion = (
        0.4 * text_cohesion + 
        0.3 * audio_cohesion + 
        0.3 * video_cohesion
    )
    
    return {
        'multimodal_cohesion': multimodal_cohesion,
        'text_weight': 0.4,
        'audio_weight': 0.3,
        'video_weight': 0.3
    }
```

## ğŸ“ˆ åˆ†æç»“æœç¤ºä¾‹

### æ–‡ä»¶çº§åˆ«VAECå¾—åˆ†
| æ–‡ä»¶å | Valence | Arousal | Energy | Cohesion | æ–‡ä»¶ç±»å‹ |
|--------|---------|---------|--------|----------|----------|
| sample_happy.wav | 0.823 | 0.756 | 0.892 | 0.000 | éŸ³é¢‘ |
| sample_positive.txt | 0.636 | 0.000 | 0.000 | 0.000 | æ–‡æœ¬ |
| sample_excited.wav | 0.456 | 0.823 | 0.945 | 0.000 | éŸ³é¢‘ |

### ä¼šè¯çº§åˆ«åˆ†æ
| ä¼šè¯ID | ä¸ªäººä¸€è‡´æ€§ | ç¯å¢ƒä¸€è‡´æ€§ | ç»¼åˆä¸€è‡´æ€§ | æ–‡ä»¶æ•° |
|--------|------------|------------|------------|--------|
| meeting_productive | 0.850 | 0.920 | 0.895 | 12 |
| meeting_stress | 0.600 | 0.750 | 0.760 | 8 |
| meeting_conflict | 0.400 | 0.300 | 0.640 | 10 |

## ğŸ› ï¸ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. **æ ¸å¿ƒæ¨¡å—**

#### VAE_CCalculatorç±»
```python
class VAE_CCalculator:
    def __init__(self):
        self.va_calculator = VACalculator()
        self.energy_calculator = EnergyCalculator()
        self.cohesion_calculator = CohesionCalculator()
    
    def calculate_vaec_scores(self, data_path: str) -> Dict[str, float]:
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è®¡ç®—æ–¹æ³•
        if data_path.endswith(('.wav', '.mp3', '.flac')):
            return self._calculate_audio_scores(data_path)
        elif data_path.endswith(('.txt', '.json')):
            return self._calculate_text_scores(data_path)
        elif data_path.endswith(('.mp4', '.avi', '.mov')):
            return self._calculate_video_scores(data_path)
```

#### MultimodalCohesionAnalyzerç±»
```python
class MultimodalCohesionAnalyzer:
    def analyze_session_cohesion(self, session_data: Dict) -> Dict:
        return {
            'text_cohesion': self._analyze_text_cohesion(session_data),
            'audio_cohesion': self._analyze_audio_cohesion(session_data),
            'video_cohesion': self._analyze_video_cohesion(session_data),
            'multimodal_cohesion': self._calculate_multimodal_cohesion(results)
        }
```

### 2. **æ•°æ®å¤„ç†æµç¨‹**

```python
def process_ami_data(self):
    # 1. æ•°æ®å½’ç±»
    self.organize_data()
    
    # 2. ç‰¹å¾æå–
    self.extract_features()
    
    # 3. VAECè®¡ç®—
    self.calculate_vaec_scores()
    
    # 4. ç»“æœèšåˆ
    self.aggregate_results()
    
    # 5. æŠ¥å‘Šç”Ÿæˆ
    self.generate_reports()
```

### 3. **å¼‚å¸¸å¤„ç†æœºåˆ¶**

```python
def safe_calculation(func):
    """è£…é¥°å™¨ï¼šå®‰å…¨è®¡ç®—ï¼Œç½‘ç»œå¤±è´¥æ—¶è‡ªåŠ¨é™çº§"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.warning(f"è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {e}")
            return simplified_calculation(*args, **kwargs)
    return wrapper
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å¤„ç†æ•ˆç‡
- **æ ·æœ¬æ•°æ®**: 13ä¸ªæ–‡ä»¶ï¼Œå¤„ç†æ—¶é—´ < 30ç§’
- **éŸ³é¢‘åˆ†æ**: æ”¯æŒå®æ—¶ç‰¹å¾æå–
- **æ–‡æœ¬åˆ†æ**: æ‰¹é‡å¤„ç†ï¼Œæ”¯æŒå¤§æ–‡ä»¶
- **è§†é¢‘åˆ†æ**: å¸§çº§å¤„ç†ï¼Œå¯é…ç½®é‡‡æ ·ç‡

### å‡†ç¡®æ€§
- **VADERæƒ…æ„Ÿåˆ†æ**: ä¸šç•Œæ ‡å‡†ï¼Œå‡†ç¡®ç‡ > 80%
- **éŸ³é¢‘ç‰¹å¾**: åŸºäºlibrosaï¼Œç§‘å­¦å¯é 
- **ä¸€è‡´æ€§è®¡ç®—**: å¤šç§ç®—æ³•ï¼Œç»“æœç¨³å®š

## ğŸ”§ é…ç½®é€‰é¡¹

### 1. **æƒé‡é…ç½®**
```python
# å¤šæ¨¡æ€ä¸€è‡´æ€§æƒé‡
MULTIMODAL_WEIGHTS = {
    'text': 0.4,
    'audio': 0.3,
    'video': 0.3
}

# ç»¼åˆä¸€è‡´æ€§æƒé‡
COHESION_WEIGHTS = {
    'personal': 0.4,
    'environmental': 0.6
}
```

### 2. **é˜ˆå€¼é…ç½®**
```python
# æƒ…æ„Ÿé˜ˆå€¼
EMOTION_THRESHOLDS = {
    'valence_positive': 0.3,
    'valence_negative': -0.3,
    'arousal_high': 0.5,
    'arousal_low': -0.5
}
```

## ğŸš€ æ‰©å±•åŠŸèƒ½

### 1. **å®æ—¶åˆ†æ**
```python
def real_time_analysis(audio_stream, video_stream, text_stream):
    """å®æ—¶å¤šæ¨¡æ€åˆ†æ"""
    while True:
        # è·å–å®æ—¶æ•°æ®
        audio_frame = audio_stream.get_frame()
        video_frame = video_stream.get_frame()
        text_chunk = text_stream.get_text()
        
        # å®æ—¶è®¡ç®—VAEC
        scores = calculate_realtime_vaec(audio_frame, video_frame, text_chunk)
        
        # è¾“å‡ºç»“æœ
        yield scores
```

### 2. **æ·±åº¦å­¦ä¹ å¢å¼º**
```python
# é›†æˆTransformeræ¨¡å‹
from transformers import pipeline

class AdvancedVACalculator:
    def __init__(self):
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment-latest"
        )
```

### 3. **APIæœåŠ¡**
```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/analyze', methods=['POST'])
def analyze_session():
    data = request.json
    analyzer = MultimodalCohesionAnalyzer()
    results = analyzer.generate_cohesion_report(data)
    return jsonify(results)
```

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### 1. **åŸºæœ¬ä½¿ç”¨**
```python
from contextvibe import VAE_CCalculator

# åˆ›å»ºè®¡ç®—å™¨
calculator = VAE_CCalculator()

# è®¡ç®—å•ä¸ªæ–‡ä»¶
scores = calculator.calculate_vaec_scores("sample_happy.wav")
print(f"Valence: {scores['valence']:.3f}")
print(f"Arousal: {scores['arousal']:.3f}")
print(f"Energy: {scores['energy']:.3f}")
print(f"Cohesion: {scores['cohesion']:.3f}")
```

### 2. **ä¼šè¯åˆ†æ**
```python
from contextvibe import MultimodalCohesionAnalyzer

# åˆ›å»ºåˆ†æå™¨
analyzer = MultimodalCohesionAnalyzer()

# åˆ†æä¼šè¯
report = analyzer.generate_cohesion_report(session_data)

# æŸ¥çœ‹ç»“æœ
summary = report['summary']
print(f"å¤šæ¨¡æ€ä¸€è‡´æ€§: {summary['multimodal_cohesion']:.3f}")
```

### 3. **æ‰¹é‡å¤„ç†**
```bash
# å¤„ç†æ•´ä¸ªæ•°æ®é›†
python3 ami_processor.py --batch_mode

# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
python3 ami_processor.py --generate_reports
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ä¾èµ–å®‰è£…å¤±è´¥**
```bash
# å‡çº§pip
pip install --upgrade pip

# å®‰è£…ç³»ç»Ÿä¾èµ–
sudo apt-get install libsndfile1-dev
```

2. **éŸ³é¢‘å¤„ç†é”™è¯¯**
```python
# æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼
import librosa
y, sr = librosa.load("audio.wav", sr=None)
print(f"é‡‡æ ·ç‡: {sr}, é•¿åº¦: {len(y)}")
```

3. **å†…å­˜ä¸è¶³**
```python
# é™ä½å¤„ç†ç²¾åº¦
librosa.load("audio.wav", sr=16000)  # é™ä½é‡‡æ ·ç‡
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è”ç³»æ–¹å¼
- **é¡¹ç›®ç»´æŠ¤**: [æ‚¨çš„é‚®ç®±]
- **é—®é¢˜åé¦ˆ**: [GitHub Issues]
- **æ–‡æ¡£æ›´æ–°**: [é¡¹ç›®Wiki]

### è´¡çŒ®æŒ‡å—
1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. å‘èµ·Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- **AMI Meeting Corpus**: æä¾›é«˜è´¨é‡çš„å¤šæ¨¡æ€ä¼šè®®æ•°æ®
- **NLTK**: æä¾›æƒ…æ„Ÿåˆ†æå·¥å…·
- **librosa**: æä¾›éŸ³é¢‘å¤„ç†åŠŸèƒ½
- **MediaPipe**: æä¾›é¢éƒ¨æ£€æµ‹åŠŸèƒ½
- **scikit-learn**: æä¾›æœºå™¨å­¦ä¹ å·¥å…·

---

**é¡¹ç›®çŠ¶æ€**: âœ… æ´»è·ƒç»´æŠ¤  
**æœ€åæ›´æ–°**: 2024å¹´8æœˆ13æ—¥  
**ç‰ˆæœ¬**: v1.0.0  
**Pythonç‰ˆæœ¬**: 3.8+
